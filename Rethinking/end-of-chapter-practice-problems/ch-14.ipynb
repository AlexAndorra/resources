{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from theano import tensor as tt\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "sns.set(context='notebook', font_scale=1.2, rc={'figure.figsize': (12, 5)})\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "\n",
    "RANDOM_SEED = 8927\n",
    "np.random.seed(286)\n",
    "\n",
    "# Helper function\n",
    "def stdz(series: pd.Series):\n",
    "    \"\"\"Standardize the given pandas Series\"\"\"\n",
    "    return (series - series.mean())/series.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14E1.\n",
    "*Add to the following model varying slopes on the predictor $x$:*\n",
    "\n",
    "$y_{i} \\sim Normal(\\mu_{i}, \\sigma)$\n",
    "\n",
    "$\\mu_{i} = \\alpha_{GROUP[i]} + \\beta x_{i}$\n",
    "\n",
    "$\\alpha_{GROUP} \\sim Normal(\\alpha, \\sigma_{\\alpha})$\n",
    "\n",
    "$\\alpha \\sim Normal(0, 10)$\n",
    "\n",
    "$\\beta \\sim Normal(0, 1)$\n",
    "\n",
    "$\\sigma \\sim HalfCauchy(2)$\n",
    "\n",
    "$\\sigma_{\\alpha} \\sim HalfCauchy(2)$\n",
    "\n",
    "Let's do it! To add a varying slope on x,we have to add a dimension to the adaptive prior. This will mean that the $\\beta$ parameter becomes the average slope, and then we’ll need a new standard deviation parameter for the slopes and a correlation parameter to estimate the correlation between intercepts and slopes. You can find an annotated example on page 407. Keep in mind that the precise notation varies among statisticians: sometimes (as below) vectors are in square brackets and matrices in parentheses, but others mix and match otherwise, or always use one or the other. As long as it is clear in context, it’ll be fine. And if anyone gives you grief about your notation, just remind (or inform) them that notational conventions vary and ask them which part requires clarification.\n",
    "\n",
    "$y_{i} \\sim Normal(\\mu_{i}, \\sigma)$\n",
    "\n",
    "$\\mu_{i} = \\alpha_{GROUP[i]} + \\beta_{GROUP[i]} x_{i}$\n",
    "\n",
    "$\\begin{bmatrix} \\alpha_{GROUP} \\\\ \\beta_{GROUP} \\end{bmatrix} \\sim MvNormal(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, S)$\n",
    "\n",
    "$S = \\begin{pmatrix} \\sigma_{\\alpha} & 0 \\\\ 0 & \\sigma_{\\beta} \\end{pmatrix}\n",
    "     R\n",
    "     \\begin{pmatrix} \\sigma_{\\alpha} & 0 \\\\ 0 & \\sigma_{\\beta} \\end{pmatrix}$\n",
    "\n",
    "$\\alpha \\sim Normal(0, 10)$\n",
    "\n",
    "$\\beta \\sim Normal(0, 10)$\n",
    "\n",
    "$\\sigma_{\\alpha} \\sim HalfCauchy(1)$\n",
    "\n",
    "$\\sigma_{\\beta} \\sim HalfCauchy(1)$\n",
    "\n",
    "$\\sigma \\sim HalfCauchy(1)$\n",
    "\n",
    "$R \\sim LKJCorr(2)$\n",
    "\n",
    "If you used different priors for $\\beta$ and $\\sigma_{\\beta}$ and $R$, that’s fine. Just be sure your choices make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat-rethink-pymc3",
   "language": "python",
   "name": "stat-rethink-pymc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
